\nonstopmode{}
\documentclass[letterpaper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\usepackage[utf8]{inputenc} % @SET ENCODING@
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `EMMA'}}
\par\bigskip{\large \today}
\end{center}
\begin{description}
\raggedright{}
\inputencoding{utf8}
\item[Type]\AsIs{Package}
\item[Title]\AsIs{Evaluation of Methods for Dealing with Missing Data in Machine
Learning Algorithms}
\item[Version]\AsIs{0.4.0}
\item[Author]\AsIs{Jan Borowski, Piotr Fic}
\item[Maintainer]\AsIs{Jan Borowski }\email{janborowka7@gmail.com}\AsIs{}
\item[Description]\AsIs{Package crate uniform interface for several advanced imputations missing data methods. Every available method can be used as a part of mlr3 pipelines whats allow easy tuning and performance evaluation. Most of the used function work separately on train and test sets ( imputation is trained on the training set and impute train data, after that imputation is again trained on test set and impute test data). This approach makes training imputation parameters unprofitable.}
\item[License]\AsIs{GPL}
\item[Depends]\AsIs{mlr3, mlr3pipelines, paradox}
\item[Imports]\AsIs{mlr3learners, missForest, missMDA, doParallel, testthat,
Amelia, VIM, softImpute, missRanger, methods, mice, data.table,
foreach}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[RoxygenNote]\AsIs{7.1.1.9000}
\item[NeedsCompilation]\AsIs{no}
\end{description}
\Rdcontents{\R{} topics documented:}
\inputencoding{utf8}
\HeaderA{autotune\_Amelia}{Perform imputation using Amelia package and EMB algorithm.}{autotune.Rul.Amelia}
%
\begin{Description}\relax
Function use EMB (Expectation-Maximization with Bootstrapping ) to impute missing data. Function performance is highly depend from data structure and
chosen parameters.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_Amelia(
  df,
  col_type,
  percent_of_missing,
  col_0_1 = FALSE,
  parallel = TRUE,
  polytime = NULL,
  splinetime = NULL,
  intercs = FALSE,
  empir = NULL,
  verbose = FALSE,
  return_one = TRUE,
  m = 3,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{parallel}] If true parallel calculation is used.

\item[\code{polytime}] parameter pass to amelia function

\item[\code{splinetime}] parameter pass to amelia finction

\item[\code{intercs}] parameter pass to amleia function

\item[\code{empir}] parameter pass to amelia function as empir in Amelia == empir*nrow(df). If empir dont set empir=nrow(df)*0.015.

\item[\code{verbose}] If true function will print on console.

\item[\code{return\_one}] Decide if one dataset or amelia object will be returned.

\item[\code{m}] Number of datasets generated by amelia. If retrun\_one=TRUE first dataset will be given.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Return one data.frame with imputed values or amelia object.
\end{Value}
%
\begin{References}\relax
James Honaker, Gary King, Matthew Blackwell (2011). Amelia II: A Program for Missing Data. Journal of Statistical Software, 45(7), 1-47. URL http://www.jstatsoft.org/v45/i07/.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_Amelia(raw_data, col_type, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_mice}{Automatical tuning of parameters and imputation using mice package.}{autotune.Rul.mice}
%
\begin{Description}\relax
Function impute missing data using mice functions. First perform  random search using linear models (generalized linear models if only
categorical values are available). Using glm its problematic. Function allows users to skip optimization in that case but it can lead to errors.
Function optimize prediction matrix and method. Other mice parameters like number of sets(m) or max number of iterations(maxit) should be set
as hight as possible for best results(higher values are required more time to perform imputation). If u chose to use one inputted dataset m is not important. More information can be found in \code{\LinkA{random\_param\_mice\_search}{random.Rul.param.Rul.mice.Rul.search}} and \code{\LinkA{formula\_creating}{formula.Rul.creating}} and \code{\LinkA{mice}{mice}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_mice(
  df,
  m = 5,
  maxit = 5,
  col_miss,
  col_no_miss,
  col_type,
  set_cor = 0.5,
  set_method = "pmm",
  percent_of_missing,
  low_corr = 0,
  up_corr = 1,
  methods_random = c("pmm"),
  iter,
  random.seed = 123,
  optimize = TRUE,
  correlation = TRUE,
  return_one = TRUE,
  col_0_1 = FALSE,
  verbose = FALSE,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data frame for imputation.

\item[\code{m}] number of sets produced by mice.

\item[\code{maxit}] maximum number of iteration for mice.

\item[\code{col\_miss}] name of columns with missing values.

\item[\code{col\_no\_miss}] character vector. Names of columns without NA.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{set\_cor}] Correlation or fraction of featurs using if optimize= False

\item[\code{set\_method}] Method used if optimize=False. If NULL default method is used (more in methods\_random section ).

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{low\_corr}] double betwen 0,1 default 0 lower boundry of correlation set.

\item[\code{up\_corr}] double between 0,1 default 1 upper boundary of correlation set. Both of these parameters work the same for a fraction of features.

\item[\code{methods\_random}] set of methods to chose. Default 'pmm'. If seted on NULL this methods are used predictive mean matching (numeric data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for unordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels).

\item[\code{iter}] number of iteration for randomSearch.

\item[\code{random.seed}] random seed.

\item[\code{optimize}] if user wont to optimize.

\item[\code{correlation}] If True correlation is using if Fales fraction of features. Default True.

\item[\code{return\_one}] One or many imputed sets will be returned. Default True.

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{verbose}] If FALSE function didn't print on console.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Return imputed datasets or mids object containing multi imputation datasets.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- mice::nhanes2

  col_type <- 1:ncol(raw_data)
  for (i in col_type) {
    col_type[i] <- class(raw_data[, i])
  }

  percent_of_missing <- 1:ncol(raw_data)
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }
  col_no_miss <- colnames(raw_data)[percent_of_missing == 0]
  col_miss <- colnames(raw_data)[percent_of_missing > 0]
  imp_data <- autotune_mice(raw_data, optimize = FALSE, iter = 2,
   col_type = col_type, percent_of_missing = percent_of_missing,
   col_no_miss = col_no_miss, col_miss = col_miss)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_missForest}{Perform imputation using missForest form missForest package.}{autotune.Rul.missForest}
%
\begin{Description}\relax
Function use missForest package for data imputation. OBBerror (more in  \code{\LinkA{autotune\_mice}{autotune.Rul.mice}}) is used to perform grid search.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_missForest(
  df,
  col_type,
  percent_of_missing,
  cores = NULL,
  ntree_set = c(100, 200, 500, 1000),
  mtry_set = NULL,
  parallel = FALSE,
  col_0_1 = FALSE,
  optimize = TRUE,
  ntree = 100,
  mtry = NULL,
  verbose = FALSE,
  maxiter = 20,
  maxnodes = NULL,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{cores}] integer.  Number of threads used by parallel calculations. By default approximately half of available CPU cores.

\item[\code{ntree\_set}] integer vector. Vector contains numbers of tree for grid search.

\item[\code{mtry\_set}] integer vector. Vector contains numbers of variables randomly sampled at each split.

\item[\code{parallel}] logical. If TRUE parallel calculation is using.

\item[\code{col\_0\_1}] decide if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False.

\item[\code{optimize}] optimize inside function

\item[\code{ntree}] ntree from missForest function

\item[\code{mtry}] mtry form missforest function

\item[\code{verbose}] If FALSE funtion didn't print on console.

\item[\code{maxiter}] maxiter form missForest function.

\item[\code{maxnodes}] maxnodes from missForest function.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function try to use parallel backend if it's possible. Half of the available cores are used or number pass as cores param. (Number of used cores can't be higher then number of variables in df. If it happened a number of cores will be set at ncol(df)-2 unless this number is <= 0 then cores =1).  To perform parallel calculation function use  \code{\LinkA{registerDoParallel}{registerDoParallel}} to create parallel backend.
Creating backend can have significant time cost so for very small df cores=1 can speed up calculation. After calculation function turns off parallel backend. \\{} \\{}   Gride search is used to chose a sample for each tree and the number of trees can be turn off. Params in grid search have significant influence on imputation quality but function should work on any reasonable values of this parameter.
\end{Details}
%
\begin{Value}
Return data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Daniel J. Stekhoven (2013). missForest: Nonparametric Missing Value Imputation using Random Forest. R package version 1.4.
Stekhoven D. J., \& Buehlmann, P. (2012). MissForest - non-parametric missing value imputation for mixed-type data. Bioinformatics, 28(1), 112-118.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_missForest(raw_data, col_type, percent_of_missing, optimize = FALSE)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_missRanger}{Perform imputation using missRenger form missRegnger package.}{autotune.Rul.missRanger}
%
\begin{Description}\relax
Function use missRenger package for data imputation. Function use OBBerror (more in missForest documentation) to perform random search.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_missRanger(
  df,
  percent_of_missing,
  maxiter = 10,
  random.seed = 123,
  mtry = NULL,
  num.trees = 500,
  verbose = F,
  col_0_1 = F,
  out_file = NULL,
  pmm.k = 5,
  optimize = T,
  iter = 10
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{maxiter}] maximum number of iteration for missRanger algorithm

\item[\code{random.seed}] random seed use in imputation

\item[\code{mtry}] sample fraction use by missRanger. This param isn't optimized automatically. If NULL default value from ranger package will be used.

\item[\code{num.trees}] number of trees. If optimize == TRUE. Param set seq(10,num.trees,iter) will be used.

\item[\code{verbose}] If FALSE function doesn't print on console.

\item[\code{col\_0\_1}] decide if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.

\item[\code{pmm.k}] Number of candidate non-missing values to sample from in the predictive meanmatching step. 0 to avoid this step. If optimize == TRUE param set sample(1:pmm.k,iter) will be used. If pmm.k==0 missRanger == missForest.

\item[\code{optimize}] If TRUE inside optimization will be performed.

\item[\code{iter}] Number of iteration for a random search.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Return data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Michael Mayer (2019). missRanger: Fast Imputation of Missing Values. R package version 2.1.0. https://CRAN.R-project.org/package=missRanger
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_missRanger(raw_data, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_softImpute}{Perform imputation using softImpute package}{autotune.Rul.softImpute}
%
\begin{Description}\relax
Function use softImpute to impute missing data it works only with numeric data. Columns with categorical values are imputed by a selected function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_softImpute(
  df,
  percent_of_missing,
  col_type,
  col_0_1 = F,
  cat_Fun = VIM::maxCat,
  lambda = 0,
  rank.max = 2,
  type = "als",
  thresh = 1e-05,
  maxit = 100,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{col\_type}] Character vector with types of columns.

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{cat\_Fun}] Function to impute categorical features. Default maxCat (mode). Can be every function with input one character vector and return atomic object.

\item[\code{lambda}] nuclear-norm regularization parameter. If lambda=0, the algorithm reverts to "hardImpute", for which convergence is typically slower. If null lambda is set automatically at the highest possible values.

\item[\code{rank.max}] This restricts the rank of the solution. Defoult 2 if set as NULL rank.max=min(dim(X))-1.

\item[\code{type}] Chose of algoritm 'als' or 'svd . Defoult 'als'.

\item[\code{thresh}] Threshold for convergence.

\item[\code{maxit}] Maximum number of iterations.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function use algorithm base on matrix whats meaning if only one numeric column exists in dataset imputation algorithm don't work. In that case, this column will be imputed using a function for categorical columns. Because of this algorithm is working properly only with at least two numeric features in the dataset. To specify column type argument col\_type is used so it's possible to forcefully use for example numeric factors in imputation. Action like this can led to errors and its not.
\end{Details}
%
\begin{Value}
Return one data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Trevor Hastie and Rahul Mazumder (2015). softImpute: Matrix Completion via Iterative Soft-Thresholded SVD. R package version 1.4. https://CRAN.R-project.org/package=softImpute
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_softImpute(raw_data, percent_of_missing, col_type)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_VIM\_hotdeck}{Hot-Deck imputation using VIM package.}{autotune.Rul.VIM.Rul.hotdeck}
%
\begin{Description}\relax
Function perform hotdeck function from VIM package. Any tunable parameters aren't available in this algorithm.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_VIM_hotdeck(df, percent_of_missing, col_0_1 = FALSE, out_file = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without  target column.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{col\_0\_1}] decide if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Return data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Alexander Kowarik, Matthias Templ (2016). Imputation with the R Package VIM. Journal of Statistical Software, 74(7), 1-16. doi:10.18637/jss.v074.i07
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_VIM_hotdeck(raw_data, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_VIM\_Irmi}{Perform imputation using VIM package and irmi function}{autotune.Rul.VIM.Rul.Irmi}
%
\begin{Description}\relax
Function use IRMI (Iterative robust model-based imputation ) to impute missing data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_VIM_Irmi(
  df,
  col_type,
  percent_of_missing,
  eps = 5,
  maxit = 100,
  step = FALSE,
  robust = FALSE,
  init.method = "kNN",
  force = FALSE,
  col_0_1 = FALSE,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{eps}] threshold for convergency

\item[\code{maxit}] maximum number of iterations

\item[\code{step}] stepwise model selection is applied when the parameter is set to TRUE

\item[\code{robust}] if TRUE, robust regression methods will be applied (it's impossible to set step=TRUE and robust=TRUE at the same time)

\item[\code{init.method}] Method for initialization of missing values (kNN or median)

\item[\code{force}] if TRUE, the algorithm tries to find a solution in any case, possible by using different robust methods automatically. (should be set FALSE for simulation)

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function can work with various different times depending on data size and structure. In some cases when selected param wouldn't work function try to run on default.  Most important param for both quality and reliability  its eps.
\end{Details}
%
\begin{Value}
Return one data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Alexander Kowarik, Matthias Templ (2016). Imputation with the R Package VIM. Journal of Statistical Software, 74(7), 1-16. doi:10.18637/jss.v074.i07
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_VIM_Irmi(raw_data, col_type, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_VIM\_kNN}{K nearest neighbor imputation using VIM package.}{autotune.Rul.VIM.Rul.kNN}
%
\begin{Description}\relax
Function perform kNN function from VIM packge.

@details  Function don't perform any inside param tuning. Users can change important param for kNN like number or nearest or aggregation functions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_VIM_kNN(
  df,
  percent_of_missing,
  k = 5,
  numFun = stats::median,
  catFun = VIM::maxCat,
  col_0_1 = FALSE,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without  target column.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{k}] Value of k use if optimize=FALSE

\item[\code{numFun}] function for aggregating the k Nearest Neighbours in the case of a numerical variable. Default median.

\item[\code{catFun}] function for aggregating the k Nearest Neighbours in the case of a categorical variable. Default mode.

\item[\code{col\_0\_1}] decide if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{References}\relax
Alexander Kowarik, Matthias Templ (2016). Imputation with the R Package VIM. Journal of Statistical Software, 74(7), 1-16. doi:10.18637/jss.v074.i07
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_VIM_kNN(raw_data, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{autotune\_VIM\_regrImp}{Perform imputation using VIM package and regressionImp function.}{autotune.Rul.VIM.Rul.regrImp}
%
\begin{Description}\relax
Function use Regression models to impute missing data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
autotune_VIM_regrImp(
  df,
  col_type,
  percent_of_missing,
  col_0_1 = F,
  robust = F,
  mod_cat = F,
  use_imputed = F,
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{col\_type}] Character vector with types of columns.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{robust}] TRUE/FALSE if robust regression should be used.

\item[\code{mod\_cat}] TRUE/FALSE if TRUE for categorical variables the level with the highest prediction probability is selected, otherwise it is sampled according to the probabilities.

\item[\code{use\_imputed}] TRUE/FALSe if TURE already imputed columns will be used to impute another.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function impute one column per iteration to allow more control of imputation. All columns with missing values can be imputed with different formulas. For every new column to imputation one of four formula is used \\{}
1. col to impute \textasciitilde{} all columns without missing  \\{}
2. col to impute \textasciitilde{} all numeric columns without missing \\{}
3. col to impute \textasciitilde{} first of columns without missing \\{}
4. col to impute \textasciitilde{} first of numeric columns without missing \\{}
For example, if formula 1 and 2 can't be used algorithm will try with formula 3. If all formula can't be used function will be stoped and error form tries with formula 4 or 3 presented. In some case, setting use\_imputed on TRUE can solve this problem but in general its lower quality of imputation.
\end{Details}
%
\begin{Value}
Return one data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Alexander Kowarik, Matthias Templ (2016). Imputation with the R Package VIM. Journal of Statistical Software, 74(7), 1-16. doi:10.18637/jss.v074.i07
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- autotune_VIM_regrImp(raw_data, col_type, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{fetch\_data}{Fetch data. Used in mice.reuse.}{fetch.Rul.data}
%
\begin{Description}\relax
 Retrieve the main imputation object when within the
`mice:::sampler` post-imputation calling environment
and return the data object (including missingness)
stored within.

\end{Description}
%
\begin{Usage}
\begin{verbatim}
fetch_data()
\end{verbatim}
\end{Usage}
%
\begin{Value}
data.frame
the original, non-imputed dataset of the mids object
\end{Value}
\inputencoding{utf8}
\HeaderA{formula\_creating}{Creating a formula for use in mice imputation evaluation.}{formula.Rul.creating}
%
\begin{Description}\relax
Function is used in \code{\LinkA{autotune\_mice}{autotune.Rul.mice}} but can be use sepraetly.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
formula_creating(df, col_miss, col_no_miss, col_type, percent_of_missing)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Data frame to impute missing values with column names.

\item[\code{col\_miss}] character vector. Names of columns with NA.

\item[\code{col\_no\_miss}] character vector. Names of columns without NA.

\item[\code{col\_type}] character vector. A vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function create a formula as follows. It creates one of the formulas its next possible formula impossible possible formula is created: \\{} 1. Numeric no missing \textasciitilde{} 3 numeric with most missing \\{} 2. Numeric no missing \textasciitilde{} all available numeric with missing \\{} 3. Numeric with less missing \textasciitilde{} 3 numeric with most missing \\{} 4. Numeric with less missing \textasciitilde{} all available numeric with missing \\{} 5. No numeric no missing \textasciitilde{} 3 most missing no numeric \\{} 6. No numeric no missing \textasciitilde{} all available no numeric with missing \\{} 7. No numeric with less missing \textasciitilde{} 3 no numeric with most missing \\{} 8. No numeric with less missing \textasciitilde{} all available no numeric with missing.
\\{} For example, if its impossible to create formula 1 and 2 formula 3 will be created but if it's possible to create formula 1 and 5 formula 1 will be created.
\end{Details}
%
\begin{Value}
List with formula object[1] and information if its no numeric value in dataset[2].
\end{Value}
%
\begin{References}\relax
Stef van Buuren, Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3), 1-67. URL https://www.jstatsoft.org/v45/i03/.
\end{References}
\inputencoding{utf8}
\HeaderA{mice.reuse}{Reuseble mice function}{mice.reuse}
%
\begin{Description}\relax
Reuse a previously fit multivariate imputation by chained equations to
impute values for previously unseen data without changing the imputation
fit (i.e. solely use the original training data to guide the imputation
models).

Note: see https://github.com/stefvanbuuren/mice/issues/32 for discussion
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mice.reuse(mids, newdata, maxit = 5, printFlag = TRUE, seed = NA)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{mids}] : mids object
An object of class mids, typically produces by a previous call to mice() or mice.mids()

\item[\code{newdata}] : data.frame
Previously unseen data of the same structur as used to generate `mids`

\item[\code{maxit}] : integer scalar
The number of additional Gibbs sampling iterations to refine the new imputations

\item[\code{printFlag}] : logical scalar
A Boolean flag. If TRUE, diagnostic information during the Gibbs sampling iterations
will be written to the command window. The default is TRUE.

\item[\code{seed}] : integer scalar
An integer that is used as argument by the set.seed() for offsetting the random
number generator. Default is to use the last seed value stored in `mids`
\end{ldescription}
\end{Arguments}
%
\begin{Value}
data : list of data.frames
the imputations of newdata


lastSeedValue : integer vector
the random seed at the end of the procedure
\end{Value}
%
\begin{Author}\relax
Patrick Rockenschaub git https://github.com/prockenschaub
\end{Author}
\inputencoding{utf8}
\HeaderA{mids.append}{Joining mice objects. Used in mice.reuse.}{mids.append}
%
\begin{Description}\relax
Append one mids object to another. Both objects are expected to have the same variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mids.append(x, y)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] mids object provides both data and specification of imputation procedure

\item[\code{y}] mids object only data information will be retained in the combined object
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
  Only the data specific aspects are copied (i.e. \$data, \$imp, \$where,
\$nmis), all other information in `y` is discarded. Therefore, only
the imputation model of `x` is kept and `y` must not contain missing
data in variables that did not have missing data in `x` (but the
reverse is allowed).

\end{Details}
%
\begin{Value}
mids object
a new mids object that contains all of `x` and the additional data in `y`
\end{Value}
\inputencoding{utf8}
\HeaderA{missMDA\_FMAD\_MCA\_PCA}{Perform imputation using MCA, PCA, or FMAD algorithm.}{missMDA.Rul.FMAD.Rul.MCA.Rul.PCA}
%
\begin{Description}\relax
Function use missMDA package to perform data imputation. Function can found the best number of dimensions for this imputation.
User can choose whether to return one imputed dataset or list or imputed datasets form Multiple Imputation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
missMDA_FMAD_MCA_PCA(
  df,
  col_type,
  percent_of_missing,
  optimize_ncp = TRUE,
  set_ncp = 2,
  col_0_1 = FALSE,
  ncp.max = 5,
  return_one = TRUE,
  random.seed = 123,
  maxiter = 998,
  coeff.ridge = 1,
  threshold = 1e-06,
  method = "Regularized",
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{optimize\_ncp}] logical. If true number of dimensions used to predict the missing entries will be optimized. If False by default ncp = 2 it's used.

\item[\code{set\_ncp}] intiger >0. Number of dimensions used by algortims. Used only if optimize\_ncp = Flase.

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{ncp.max}] integer corresponding to the maximum number of components to test. Default 5.

\item[\code{return\_one}] One or many imputed sets will be returned. Default True.

\item[\code{random.seed}] random seed.

\item[\code{maxiter}] maximal number of iteration in algortihm.

\item[\code{coeff.ridge}] Value use in Regularized method.

\item[\code{threshold}] threshold for convergence.

\item[\code{method}] method used in imputation algoritm.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function use different algorithm to adjust for variable types in df. For only numeric data PCA will be used. MCA for only categorical and FMAD for mixed. If optimize==TRUE function will try to find optimal ncp if its not possible default ncp=2 will be used. In some cases ncp=1 will be used if ncp=2 don't work. For multiple imputations, if set ncp don't work error will be return.
\end{Details}
%
\begin{Value}
Retrun one imputed data.frame if retrun\_one=True or list of imputed data.frames if retrun\_one=False.
\end{Value}
%
\begin{References}\relax
Julie Josse, Francois Husson (2016). missMDA: A Package for Handling Missing Values in Multivariate Data Analysis. Journal of Statistical Software, 70(1), 1-31. doi:10.18637/jss.v070.i01
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- missMDA_FMAD_MCA_PCA(raw_data, col_type, percent_of_missing, optimize_ncp = FALSE)
  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{missMDA\_MFA}{Perform imputation using MFA algorithm.}{missMDA.Rul.MFA}
%
\begin{Description}\relax
Function use MFA (Multiple Factor Analysis) to impute missing data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
missMDA_MFA(
  df,
  col_type,
  percent_of_missing,
  random.seed = 123,
  ncp = 2,
  col_0_1 = F,
  maxiter = 1000,
  coeff.ridge = 1,
  threshold = 1e-06,
  method = "Regularized",
  out_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] data.frame. Df to impute with column names and without target column.

\item[\code{col\_type}] character vector. Vector containing column type names.

\item[\code{percent\_of\_missing}] numeric vector. Vector contatining percent of missing data in columns for example  c(0,1,0,0,11.3,..)

\item[\code{random.seed}] random seed.

\item[\code{ncp}] Number of dimensions used by algorithm. Default 2.

\item[\code{col\_0\_1}] Decaid if add bonus column informing where imputation been done. 0 - value was in dataset, 1 - value was imputed. Default False. (Works only for returning one dataset).

\item[\code{maxiter}] maximal number of iteration in algorithm.

\item[\code{coeff.ridge}] Value use in Regularized method.

\item[\code{threshold}] for convergence.

\item[\code{method}] used in imputation algorithm.

\item[\code{out\_file}] Output log file location if file already exists log message will be added. If NULL no log will be produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Groups are created using the original column order and taking as much variable to one group as possible. MFA requires selecting group type but numeric types can only be set as 'c' - centered and 's' - scale to unit variance.
It's impossible to provide these conditions so numeric type is always set as 's'.  Because of that imputation can depend from column order. In this function, no param is set automatically but if selected ncp don't work function will try use ncp=1.
\end{Details}
%
\begin{Value}
Return one data.frame with imputed values.
\end{Value}
%
\begin{References}\relax
Julie Josse, Francois Husson (2016). missMDA: A Package for Handling Missing Values in Multivariate Data Analysis. Journal of Statistical Software, 70(1), 1-31. doi:10.18637/jss.v070.i01
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
{
  raw_data <- data.frame(
    a = as.factor(sample(c("red", "yellow", "blue", NA), 1000, replace = TRUE)),
    b = as.integer(1:1000),
    c = as.factor(sample(c("YES", "NO", NA), 1000, replace = TRUE)),
    d = runif(1000, 1, 10),
    e = as.factor(sample(c("YES", "NO"), 1000, replace = TRUE)),
    f = as.factor(sample(c("male", "female", "trans", "other", NA), 1000, replace = TRUE)))

  # Prepering col_type
  col_type <- c("factor", "integer", "factor", "numeric", "factor", "factor")

  percent_of_missing <- 1:6
  for (i in percent_of_missing) {
    percent_of_missing[i] <- 100 * (sum(is.na(raw_data[, i])) / nrow(raw_data))
  }


  imp_data <- missMDA_MFA(raw_data, col_type, percent_of_missing)

  # Check if all missing value was imputed
  sum(is.na(imp_data)) == 0
  # TRUE
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpAmelia}{PipeOpAmelia}{PipeOpAmelia}
%
\begin{Description}\relax
Implements EMB methods as mlr3 pipeline more about Amelia \code{\LinkA{autotune\_Amelia}{autotune.Rul.Amelia}} or \url{https://cran.r-project.org/package=Amelia}
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_Amelia"}.
\item{} \code{m} :: \code{integer(1)}\\{}
Number of datasets generated by Amelia, default \code{3}.
\item{} \code{polytime} :: \code{integer(1)}\\{}
Integer between 0 and 3 indicating what power of polynomial should be included in the imputation model to account for the effects of time. A setting of 0 would indicate constant levels, 1 would indicate linear time effects, 2 would indicate squared effects, and 3 would indicate cubic time effects, default \code{NULL}.
\item{} \code{splinetime} :: \code{integer(1)}\\{}
Integer value of 0 or greater to control cubic smoothing splines of time. Values between 0 and 3 create a simple polynomial of time (identical to the polytime argument). Values k greater than 3 create a spline with an additional k-3 knotpoints, default \code{NULL}.
\item{} \code{intercs} :: \code{logical(1)}\\{}
Variable indicating if the time effects of polytime should vary across the cross-section, default \code{FALSE}.
\item{} \code{empir} :: \code{double(1)}\\{}
Number indicating level of the empirical (or ridge) prior. This prior shrinks the covariances of the data, but keeps the means and variances the same for problems of high missingness, small N's or large correlations among the variables. Should be kept small, perhaps 0.5 to 1 percent of the rows of the data; a reasonable upper bound is around 10 percent of the rows of the data. If empir is not set, empir=nrow(df)*0.015, default \code{NULL}.
\item{} \code{parallel} :: \code{double(1)}\\{}
If true parallel calculation is used, default \code{TRUE}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Amelia\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpAmelia\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpAmelia\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpAmelia$new(
  id = "impute_Amelia_B",
  polytime = NULL,
  splinetime = NULL,
  intercs = FALSE,
  empir = NULL,
  m = 3,
  parallel = TRUE,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpAmelia$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpAmelia$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpHist\_B}{PipeOpHist\_B}{PipeOpHist.Rul.B}
%
\begin{Description}\relax
Impute numerical features by histogram in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default `"impute\_hist\_B"`.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Hist\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpHist\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpHist\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpHist_B$new(id = "impute_hist_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpHist_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpHist_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpMean\_B}{PipeOpMean\_B}{PipeOpMean.Rul.B}
%
\begin{Description}\relax
Impute numerical features by their mean in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_mean\_B"}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Mean\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMean\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMean\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMean_B$new(id = "impute_mean_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMean_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMean_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpMedian\_B}{PipeOpMedian\_B}{PipeOpMedian.Rul.B}
%
\begin{Description}\relax
Impute features by OOR imputation in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default `"impute\_median\_B"`.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Median\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMedian\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMedian\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMedian_B$new(id = "impute_median_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMedian_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMedian_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpMice}{PipeOpMice}{PipeOpMice}
%
\begin{Description}\relax
Implements mice methods as mlr3 pipeline more about mice \code{\LinkA{autotune\_mice}{autotune.Rul.mice}}
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_mice"}.
\item{} \code{m} :: \code{integer(1)}\\{}
Number of datasets produced by mice, default \code{5}.
\item{} \code{maxit} :: \code{integer(1)}\\{}
Maximum number of iterations for mice, default \code{5}.
\item{} \code{set\_corr} :: \code{double(1)}\\{}
Correlation or fraction of features used when optimize=FALSE. When correlation=FALSE, it represents a fraction of case to use in imputation for each variable, default \code{0.5}.
\item{} \code{set\_method} :: \code{character(1)}\\{}
Method used if optimize=FALSE. If NULL default method is used (more in methods\_random section), default \code{'pmm'}.
\item{} \code{low\_corr} :: \code{double(1)}\\{}
Double between 0-1. Lower boundary of correlation used in inner optimization (used only when optimize=TRUE), default \code{0}.
\item{} \code{up\_corr} :: \code{double(1)}\\{}
Double between 0-1. Upper boundary of correlation used in inner optimization (used only when optimize=TRUE). Both of these parameters work the same for a fraction of case if correlation=FALSE,default \code{1}.
\item{} \code{methods\_random} :: \code{character(1)}\\{}
set of methods to chose. Avalible methods "pmm", "midastouch", "sample", "cart", "rf" Default 'pmm'. If seted on NULL this methods are used predictive mean matching (numeric data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for unordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels).
\item{} \code{iter} :: \code{integer(1)}\\{}
Number of iteration for random search, default \code{5}.
\item{} \code{random.seed} :: \code{integer(1)}\\{}
Random seed, default \code{123}.
\item{} \code{optimize} :: \code{logical(1)}\\{}
If set TRUE, function will optimize parameters of imputation automatically. If parameters will be tuned by other method, should be set to FALSE, default \code{FALSE}.
\item{} \code{correlation} :: \code{logical(1)}\\{}
If set TRUE correlation is used, if set FALSE then fraction of case, default \code{TRUE}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{mice\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMice\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMice\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMice$new(
  id = "impute_mice_B",
  m = 5,
  maxit = 5,
  set_cor = 0.5,
  set_method = "pmm",
  low_corr = 0,
  up_corr = 1,
  methods_random = c("pmm"),
  iter = 5,
  random.seed = 123,
  optimize = F,
  correlation = F,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMice$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMice$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpMice\_A}{PipeOpMice\_A}{PipeOpMice.Rul.A}
%
\begin{Description}\relax
Implements mice methods as mlr3 in A approach (training imputation model on training data and used a trained model on test data).
\end{Description}
%
\begin{Details}\relax
Code of used function was writen by \url{https://github.com/prockenschaub} more information aboute this aproche can be found here \url{https://github.com/amices/mice/issues/32}
\end{Details}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_mice\_A"}.
\item{} \code{m} :: \code{integer(1)}\\{}
Number of datasets produced by mice, default \code{5}.
\item{} \code{maxit} :: \code{integer(1)}\\{}
Maximum number of iterations for mice, default \code{5}.
\item{} \code{set\_corr} :: \code{double(1)}\\{}
Correlation or fraction of features used when optimize=FALSE. When correlation=FALSE, it represents a fraction of case to use in imputation for each variable, default \code{0.5}.
\item{} \code{random.seed} :: \code{integer(1)}\\{}
Random seed, default \code{123}.
\item{} \code{correlation} :: \code{logical(1)}\\{}
If set TRUE correlation is used, if set FALSE then fraction of case, default \code{TRUE}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{mice\_A\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMice\_A\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMice\_A\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMice_A$new(
  id = "impute_mice_A",
  set_cor = 0.5,
  m = 5,
  maxit = 5,
  random.seed = 123,
  correlation = F,
  methods = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMice_A$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMice_A$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpmissForest}{PipeOpmissForest}{PipeOpmissForest}
%
\begin{Description}\relax
Implements missForest methods as mlr3 pipeline more about missForest \code{\LinkA{autotune\_missForest}{autotune.Rul.missForest}}
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_missForest"}.
\item{} \code{cores} :: \code{integer(1)}\\{}
Number of threads used by parallel calculations. If NULL approximately half of available CPU cores will be used, default \code{NULL}.
\item{} \code{ntree\_set} :: \code{integer(1)}\\{}
Vector with \emph{number of trees} values for grid search, used only when optimize=TRUE, default \code{c(100,200,500,1000)}.
\item{} \code{mtry\_set} :: \code{integer(1)}\\{}
Vector with \emph{number of variables} values randomly sampled at each split, used only when optimize=TRUE, default \code{NULL}.
\item{} \code{parallel} :: \code{logical(1)}\\{}
If TRUE parallel calculations are used, default \code{FALSE}.
\item{} \code{ntree} :: \code{integer(1)}\\{}
ntree from missForest function, default \code{100}.
\item{} \code{optimize} :: \code{logical(1)}\\{}
If set TRUE, function will optimize parameters of imputation automatically. If parameters will be tuned by other method, should be set to FALSE, default \code{FALSE}.
\item{} \code{mtry} :: \code{integer(1)}\\{}
mtry from missForest function, default \code{NULL}.
\item{} \code{maxiter} :: \code{integer(1)}\\{}
maxiter from missForest function, default \code{20}.
\item{} \code{maxnodes} :: \code{character(1)}\\{}
maxnodes from missForest function, default \code{NULL}
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{missForest\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpmissForest\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpmissForest\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpmissForest$new(
  id = "impute_missForest_B",
  cores = NULL,
  ntree_set = c(100, 200, 500, 1000),
  mtry_set = NULL,
  parallel = F,
  mtry = NULL,
  ntree = 100,
  optimize = FALSE,
  maxiter = 20,
  maxnodes = NULL,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpmissForest$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
  graph <- PipeOpmissForest$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))

## End(Not run)
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpmissMDA\_MFA}{PipeOpmissMDA\_MFA}{PipeOpmissMDA.Rul.MFA}
\aliasA{PipeOpMissMDA\_MFA}{PipeOpmissMDA\_MFA}{PipeOpMissMDA.Rul.MFA}
%
\begin{Description}\relax
Implements MFA methods as mlr3 pipeline, more about MFA \code{\LinkA{missMDA\_MFA}{missMDA.Rul.MFA}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_missMDA\_MFA"}.
\item{} \code{ncp} :: \code{integer(1)}\\{}
Number of dimensions used by algorithm, default \code{2}.
\item{} \code{random.seed} :: \code{integer(1)}\\{}
Random seed, default \code{123}.
\item{} \code{maxiter} :: \code{integer(1)}\\{}
Maximal number of iteration in algorithm, default \code{998}.
\item{} \code{coeff.ridge} :: \code{integer(1)}\\{}
Value used in \emph{Regularized} method, default \code{1}.
\item{} \code{threshold} :: \code{double(1)}\\{}
Threshold for convergence, default \code{1e-06}.
\item{} \code{method} :: \code{character(1)}\\{}
Method used in imputation algorithm, default \code{'Regularized'}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{missMDA\_MFAimputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMissMDA\_MFA\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMissMDA\_MFA\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMissMDA_MFA$new(
  id = "impute_missMDA_MFA_B",
  ncp = 2,
  random.seed = 123,
  maxiter = 998,
  coeff.ridge = 1,
  threshold = 1e-06,
  method = "Regularized",
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMissMDA_MFA$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMissMDA_MFA$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpmissMDA\_PCA\_MCA\_FMAD}{PipeOpmissMDA\_PCA\_MCA\_FMAD}{PipeOpmissMDA.Rul.PCA.Rul.MCA.Rul.FMAD}
\aliasA{PipeOpMissMDA\_PCA\_MCA\_FMAD}{PipeOpmissMDA\_PCA\_MCA\_FMAD}{PipeOpMissMDA.Rul.PCA.Rul.MCA.Rul.FMAD}
%
\begin{Description}\relax
Implements PCA, MCA, FMAD methods as mlr3 pipeline, more about methods \code{\LinkA{missMDA\_FMAD\_MCA\_PCA}{missMDA.Rul.FMAD.Rul.MCA.Rul.PCA}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_missMDA\_MCA\_PCA\_FMAD"}.
\item{} \code{optimize\_ncp} :: \code{logical(1)}\\{}
If TRUE, parameter \emph{number of dimensions}, used to predict the missing values, will be optimized. If FALSE, by default ncp=2 is used, default \code{TRUE}.
\item{} \code{set\_ncp} :: \code{integer(1)}\\{}
integer >0. Number of dimensions used by algortims. Used only if optimize\_ncp = Flase, default \code{2}.
\item{} \code{ncp.max} :: \code{integer(1)}\\{}
Number corresponding to the maximum number of components to test when optimize\_ncp=TRUE, default \code{5}.
\item{} \code{random.seed} :: \code{integer(1)}\\{}
Random seed, default \code{123}.
\item{} \code{maxiter} :: \code{integer(1)}\\{}
Maximal number of iteration in algorithm, default \code{998}.
\item{} \code{coeff.ridge} :: \code{double(1)}\\{}
Value used in \emph{Regularized} method, default \code{1}.
\item{} \code{threshold} :: \code{double(1)}\\{}
Threshold for convergence, default \code{1e-6}.
\item{} \code{method} :: \code{character(1)}\\{}
Method used in imputation algorithm, default \code{'Regularized'}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{missMDA\_MCA\_PCA\_FMAD\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMissMDA\_PCA\_MCA\_FMAD\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMissMDA\_PCA\_MCA\_FMAD\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMissMDA_PCA_MCA_FMAD$new(
  id = "impute_missMDA_MCA_PCA_FMAD_B",
  optimize_ncp = T,
  set_ncp = 2,
  ncp.max = 5,
  random.seed = 123,
  maxiter = 998,
  coeff.ridge = 1,
  threshold = 1e-06,
  method = "Regularized",
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMissMDA_PCA_MCA_FMAD$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMissMDA_PCA_MCA_FMAD$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA
  set.seed(1)
  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpmissRanger}{PipeOpmissRanger}{PipeOpmissRanger}
%
\begin{Description}\relax
Implements missRanger methods as mlr3 pipeline, more about missRanger \code{\LinkA{autotune\_missRanger}{autotune.Rul.missRanger}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_missRanger"}.
\item{} \code{mtry} :: \code{integer(1)}\\{}
Sample fraction used by missRanger. This param isn't optimized automatically. If NULL default value from ranger package will be used, \code{NULL}.
\item{} \code{num.trees} :: \code{integer(1)}\\{}
Number of trees. If optimize == TRUE. Param set seq(10,num.trees,iter) will be used, default \code{500}
\item{} \code{pmm.k} :: \code{integer(1)}\\{}
Number of candidate non-missing values to sample from in the predictive mean matching step. 0 to avoid this step. If optimize=TRUE params set: sample(1:pmm.k, iter) will be used. If pmm.k=0, missRanger is the same as missForest, default \code{5}.
\item{} \code{random.seed} :: \code{integer(1)}\\{}
Random seed, default \code{123}.
\item{} \code{iter} :: \code{integer(1)}\\{}
Number of iterations for a random search, default \code{10}.
\item{} \code{optimize} :: \code{logical(1)}\\{}
If set TRUE, function will optimize parameters of imputation automatically. If parameters will be tuned by other method, should be set to FALSE, default \code{FALSE}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{missRanger\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpmissRanger\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpmissRanger\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpmissRanger$new(
  id = "impute_missRanger_B",
  maxiter = 10,
  random.seed = 123,
  mtry = NULL,
  num.trees = 500,
  pmm.k = 5,
  optimize = F,
  iter = 10,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpmissRanger$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
  graph <- PipeOpmissRanger$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))

## End(Not run)
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpMode\_B}{PipeOpMode\_B}{PipeOpMode.Rul.B}
%
\begin{Description}\relax
Impute features by their mode in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default `"impute\_mode\_B"`.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Mode\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpMode\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpMode\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMode_B$new(id = "impute_mode_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpMode_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpMode_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpOOR\_B}{PipeOpOOR\_B}{PipeOpOOR.Rul.B}
%
\begin{Description}\relax
Impute features by OOR imputation in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default `"impute\_OOR\_B"`.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{OOR\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpOOR\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpOOR\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpOOR_B$new(id = "impute_oor_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpOOR_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpOOR_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpSample\_B}{PipeOpSample\_B}{PipeOpSample.Rul.B}
%
\begin{Description}\relax
Impute features by sampling from non-missing data in approach B (independently during the training and prediction phase).
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default `"impute\_sample\_B"`.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{Sample\_B\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpSample\_B\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpSample\_B\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSample_B$new(id = "impute_sample_B", param_vals = list())\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSample_B$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpSample_B$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpSimulateMissings}{PipeOpSimulateMissings}{PipeOpSimulateMissings}
%
\begin{Description}\relax
Generates MCAR missing values in mlr3 pipeline according to set parameters.
Missings are inserted to task data once during first training.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpTaskPreproc}{PipeOpTaskPreproc}}.
\end{Section}
%
\begin{Section}{Parameters}

\begin{itemize}

\item{} \code{per\_missings} :: \code{double(1)}\\{}
Overall percentage of missing values generated in dataset [0, 100]. Must be set every time, default 50
\item{} \code{per\_instances\_missings} :: \code{double(1)}\\{}
Percentage of instances which will have missing values [0, 100].
\item{} \code{per\_variables\_missings} :: \code{double(1)}\\{}
Percentage of variables which will have missing values [0, 100].
\item{} \code{variables\_missings} :: \code{integer}\\{}
Only when `per\_variables\_missings` is `NULL`. Vector of indexes of columns in which missings will be generated.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpTaskPreproc}{mlr3pipelines::PipeOpTaskPreproc}} -> \code{PipeOpSimulateMissings}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpSimulateMissings\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpSimulateMissings\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSimulateMissings$new(
  id = "simulate_missings",
  param_vals = list(per_missings = 50)
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSimulateMissings$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  task_NA <- PipeOpSimulateMissings$new()$train(list(tsk("iris")))[[1]]

  # check
  sum(task_NA$missings()) > 0
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpSoftImpute}{PipeOpSoftImpute}{PipeOpSoftImpute}
%
\begin{Description}\relax
Implements SoftImpute methods as mlr3 pipeline, more about SoftImpute \code{\LinkA{autotune\_softImpute}{autotune.Rul.softImpute}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_softImpute"}.
\item{} \code{lambda} :: \code{integer(1)}\\{}
Nuclear-norm regularization parameter. If lambda=0, the algorithm reverts to "hardImpute", for which convergence is typically slower. If NULL lambda is set automatically at the highest possible value, default \code{0}.
\item{} \code{rank.max} :: \code{integer(1)}\\{}
This param restricts the rank of the solution. If set as NULL: rank.max=min(dim(X))-1, default \code{2}.
\item{} \code{type} :: \code{character(1)}\\{}
Two algorithms are implemented: type="svd" or the default type="als". The "svd" algorithm repeatedly computes the svd of the completed matrix, and soft thresholds its singular values. Each new soft-thresholded svd is used to re-impute the missing entries. For large matrices of class "Incomplete", the svd is achieved by an efficient form of alternating orthogonal ridge regression. The "als" algorithm uses the same alternating ridge regression, but updates the imputation at each step, leading to quite substantial speedups in some cases. The "als" approach does not currently have the same theoretical convergence guarantees as the "svd" approach, default \code{'als'}.
\item{} \code{thresh} :: \code{double(1)}\\{}
Threshold for convergence, default \code{1e-5}
\item{} \code{maxit} :: \code{integer(1)}\\{}
Maximum number of iterations, default \code{100}.
\item{} \code{cat\_Fun} :: \code{function()\{\}}\\{}
Function for aggregating the k Nearest Neighbors in case of categorical variables. It can be any function with input=not\_numeric\_vector and output=atomic\_object, default \code{VIM::maxCat}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{softImpute\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpSoftImpute\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpSoftImpute\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSoftImpute$new(
  id = "impute_softImpute_B",
  cat_Fun = VIM::maxCat,
  lambda = 0,
  rank.max = 2,
  type = "als",
  thresh = 1e-05,
  maxit = 100,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpSoftImpute$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpAmelia$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpVIM\_HD}{PipeOpVIM\_HD}{PipeOpVIM.Rul.HD}
%
\begin{Description}\relax
Implements Hot Deck methods as mlr3 pipeline more about VIM\_HD \code{\LinkA{autotune\_VIM\_hotdeck}{autotune.Rul.VIM.Rul.hotdeck}}
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_VIM\_HD"}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{VIM\_HD\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpVIM\_HD\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpVIM\_HD\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_HD$new(id = "impute_VIM_HD_B", out_file = NULL)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_HD$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpVIM_HD$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpVIM\_IRMI}{PipeOpVIM\_IRMI}{PipeOpVIM.Rul.IRMI}
%
\begin{Description}\relax
Implements IRMI methods as mlr3 pipeline, more about VIM\_IRMI \code{\LinkA{autotune\_VIM\_Irmi}{autotune.Rul.VIM.Rul.Irmi}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_VIM\_IRMI"}.
\item{} \code{eps} :: \code{double(1)}\\{}
Threshold for convergence, default \code{5}.
\item{} \code{maxit} :: \code{integer(1)}\\{}
Maximum number of iterations, default \code{100}
\item{} \code{step} :: \code{logical(1)}\\{}
Stepwise model selection is applied when the parameter is set to TRUE, default \code{FALSE}.
\item{} \code{robust} :: \code{logical(1)}\\{}
If TRUE, robust regression methods will be applied (it's impossible to set step=TRUE and robust=TRUE at the same time), default \code{FALSE}.
\item{} \code{init.method} :: \code{character(1)}\\{}
Method for initialization of missing values (kNN or median), default \code{'kNN'}.
\item{} \code{force} :: \code{logical(1)}\\{}
If TRUE, the algorithm tries to find a solution in any case by using different robust methods automatically (should be set FALSE for simulation), default \code{FALSE}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{VIM\_IRMI\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpVIM\_IRMI\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpVIM\_IRMI\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_IRMI$new(
  id = "impute_VIM_IRMI_B",
  eps = 5,
  maxit = 100,
  step = FALSE,
  robust = FALSE,
  init.method = "kNN",
  force = FALSE,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_IRMI$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpVIM_IRMI$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpVIM\_kNN}{PipeOpVIM\_kNN}{PipeOpVIM.Rul.kNN}
%
\begin{Description}\relax
Implements KNN methods as mlr3 pipeline, more about VIM\_KNN \code{\LinkA{autotune\_VIM\_kNN}{autotune.Rul.VIM.Rul.kNN}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_VIM\_kNN"}.
\item{} \code{k} :: \code{intiger(1)}\\{}
Threshold for convergence, default \code{5}.
\item{} \code{numFUN} :: \code{function()\{\}}\\{}
Function for aggregating the k Nearest Neighbours in the case of a numerical variable.  Can be ever function with input=numeric\_vector and output=atomic\_object, default \code{median}.
\item{} \code{catFUN} :: \code{function()\{\}}\\{}
Function for aggregating the k Nearest Neighbours in case of categorical variables. It can be any function with input=not\_numeric\_vector and output=atomic\_object, default \code{VIM::maxCat}
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{VIM\_kNN\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpVIM\_kNN\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpVIM\_kNN\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_kNN$new(
  id = "impute_VIM_kNN_B",
  k = 5,
  numFun = median,
  catFun = VIM::maxCat,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_kNN$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpVIM_kNN$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{PipeOpVIM\_regrImp}{PipeOpVIM\_regrImp}{PipeOpVIM.Rul.regrImp}
%
\begin{Description}\relax
Implements Regression Imputation methods as mlr3 pipeline, more about RI \code{\LinkA{autotune\_VIM\_regrImp}{autotune.Rul.VIM.Rul.regrImp}}.
\end{Description}
%
\begin{Section}{Input and Output Channels}

Input and output channels are inherited from \code{\LinkA{PipeOpImpute}{PipeOpImpute}}.
\end{Section}
%
\begin{Section}{Parameters}

The parameters include inherited from [`PipeOpImpute`], as well as: \\{}
\begin{itemize}

\item{} \code{id} :: \code{character(1)}\\{}
Identifier of resulting object, default \code{"imput\_VIM\_regrImp"}.
\item{} \code{robust} :: \code{logical(1)}\\{}
TRUE/FALSE: whether to use robust regression, default \code{FALSE}.
\item{} \code{mod\_cat} :: \code{logical(1)}\\{}
TTRUE/FALSE if TRUE for categorical variables the level with the highest prediction probability is selected, otherwise it is sampled according to the probabilities, default \code{FALSE}.
\item{} \code{use\_imputed} :: \code{logical(1)}\\{}
TRUE/FALSe: if TURE, already imputed columns will be used to impute others, default \code{FALSE}.
\item{} \code{out\_fill} :: \code{character(1)}\\{}
Output log file location. If file already exists log message will be added. If NULL no log will be produced, default \code{NULL}.

\end{itemize}

\end{Section}
%
\begin{Section}{Super classes}
\code{\LinkA{mlr3pipelines::PipeOp}{mlr3pipelines::PipeOp}} -> \code{\LinkA{mlr3pipelines::PipeOpImpute}{mlr3pipelines::PipeOpImpute}} -> \code{VIM\_regrImp\_imputation}
\end{Section}
%
\begin{Section}{Methods}
%
\begin{SubSection}{Public methods}
\begin{itemize}

\item{} \Rhref{#method-new}{\code{PipeOpVIM\_regrImp\$new()}}
\item{} \Rhref{#method-clone}{\code{PipeOpVIM\_regrImp\$clone()}}

\end{itemize}

\end{SubSection}




\hypertarget{method-new}{}
%
\begin{SubSection}{Method \code{new()}}
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_regrImp$new(
  id = "impute_VIM_regrImp_B",
  robust = FALSE,
  mod_cat = FALSE,
  use_imputed = FALSE,
  out_file = NULL
)\end{alltt}

\end{SubSubSection}


\end{SubSection}



\hypertarget{method-clone}{}
%
\begin{SubSection}{Method \code{clone()}}
The objects of this class are cloneable with this method.
%
\begin{SubSubSection}{Usage}
\begin{alltt}PipeOpVIM_regrImp$clone(deep = FALSE)\end{alltt}

\end{SubSubSection}


%
\begin{SubSubSection}{Arguments}

\begin{description}

\item[\code{deep}] Whether to make a deep clone.

\end{description}


\end{SubSubSection}

\end{SubSection}

\end{Section}
%
\begin{Examples}
\begin{ExampleCode}
{
  graph <- PipeOpVIM_regrImp$new() %>>% mlr3learners::LearnerClassifGlmnet$new()
  graph_learner <- GraphLearner$new(graph)

  # Task with NA

  resample(tsk("pima"), graph_learner, rsmp("cv", folds = 3))
}
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{random\_param\_mice\_search}{Performing randomSearch for selecting the best method and correlation or fraction of features used to create a prediction matrix.}{random.Rul.param.Rul.mice.Rul.search}
%
\begin{Description}\relax
This function perform random search and return values corresponding to best mean MIF (missing information fraction). Function is mainly used in \code{\LinkA{autotune\_mice}{autotune.Rul.mice}} but can be use separately.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
random_param_mice_search(
  low_corr = 0,
  up_corr = 1,
  methods_random = c("pmm"),
  df,
  formula,
  no_numeric,
  iter,
  random.seed = 123,
  correlation = T
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{low\_corr}] double between 0,1 default 0 lower boundry of correlation set.

\item[\code{up\_corr}] double between 0,1 default 1 upper boundary of correlation set. Both of these parameters work the same for a fraction of features.

\item[\code{methods\_random}] set of methods to chose. Default 'pmm'.

\item[\code{df}] data frame to input.

\item[\code{formula}] first product of formula\_creating() funtion. For example formula\_creating(...)[1]

\item[\code{no\_numeric}] second product of formula\_creating() function.

\item[\code{iter}] number of iteration for randomSearch.

\item[\code{random.seed}] radnom seed.

\item[\code{correlation}] If True correlation is using if Fales fraction of features. Default True.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Function use Random Search Technik to found the best param for mice imputation. To evaluate the next iteration logistic regression or linear regression (depending on available features) are used. Model is build using a formula from \code{\LinkA{formula\_creating}{formula.Rul.creating}} function. As metric MIF (missing information fraction) is used. Params combination with lowest (best) MIF is chosen. Even if a correlation is set at False correlation it's still used to select the best features. That main problem with
calculating correlation between categorical columns is still important.
\end{Details}
%
\begin{Value}
List with best correlation (or fraction ) at first place, best method at second, and results of every iteration at 3.
\end{Value}
\inputencoding{utf8}
\HeaderA{replace\_overimputes}{Replace overimputes. Used in mice.reuse.}{replace.Rul.overimputes}
%
\begin{Description}\relax
 Replace all overimputed data points in the mice imputation
of one variable. Overimputed data points are those data
that were not missing in the original but were marked for
imputation manually and imputed by the imputation procedure.

\end{Description}
%
\begin{Usage}
\begin{verbatim}
replace_overimputes(data, imp, j, i)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] data.frame the original, non-imputed dataset (mids\$data)

\item[\code{imp}] list of data.frames all imputations stored in the mids object

\item[\code{j}] character scalar the name of the variable whose imputations should be replaced

\item[\code{i}] character or integer scalar the number of the current imputation (can be 1:m)
\end{ldescription}
\end{Arguments}
\inputencoding{utf8}
\HeaderA{simulate\_missings}{Generate MCAR missings in dataset.}{simulate.Rul.missings}
%
\begin{Description}\relax
Function generates random missing values in given dataset
according to set parameters.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
simulate_missings(
  df,
  per_missings,
  per_instances_missings = NULL,
  per_variables_missings = NULL,
  variables_with_missings = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] Data.frame or data.table where missing values will be generated

\item[\code{per\_missings}] Overall percentage of missing values generated in dataset. Must be set every time.

\item[\code{per\_instances\_missings}] Percentage of instances which will have missing values.

\item[\code{per\_variables\_missings}] Percentage of variables which will have missing values.

\item[\code{variables\_with\_missings}] Only when `per\_variables\_missings` is `NULL`.
Vector of column indexes where missings will be generated.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Dataset with generated missings.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
{
  data_NA <- simulate_missings(iris, 20)

  # check
  sum(is.na(data_NA)) > 0
}
\end{ExampleCode}
\end{Examples}
\printindex{}
\end{document}
